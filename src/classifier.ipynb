{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ef49df",
   "metadata": {
    "id": "d7ef49df"
   },
   "source": [
    "# 질문 분류기 (Question Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1432db6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1432db6c",
    "outputId": "a768a7e8-d93a-4072-e29a-d4cf92945744"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: line 1: 2.0: No such file or directory\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m137.6/137.6 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.0/9.0 MB\u001B[0m \u001B[31m118.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m101.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m244.2/244.2 kB\u001B[0m \u001B[31m18.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m97.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m71.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m50.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m14.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m44.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m906.5/906.5 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m209.5/209.5 MB\u001B[0m \u001B[31m5.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!pip install -q numpy<2.0\n",
    "!pip install -q datasets pandas\n",
    "!pip install -q transformers==4.40.0\n",
    "!pip install -q accelerate==0.21.0\n",
    "!pip install -q torch==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf788f91",
   "metadata": {
    "id": "cf788f91",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "49fcde95-fe96-4aa7-f9aa-532bd1d93bf7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f278563d",
   "metadata": {
    "id": "f278563d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4594300b-2247-4276-e80c-22c5fb3c9ebe"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class QClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(QClassifier, self).__init__()\n",
    "        self.basemodel = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.basemodel.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.basemodel(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "model = QClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ecf626",
   "metadata": {
    "id": "88ecf626"
   },
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, json_path, tokenizer, max_len=64):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            item[\"question\"],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
    "            \"token_type_ids\": encoded[\"token_type_ids\"].squeeze(),\n",
    "            \"label\": torch.tensor(item[\"label\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5767f048",
   "metadata": {
    "id": "5767f048"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 원본 전체 데이터 로드\n",
    "with open(\"./data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# 80% train, 20% validation split\n",
    "train_data, val_data = train_test_split(full_data, test_size=0.2, stratify=[x[\"label\"] for x in full_data], random_state=42)\n",
    "\n",
    "# 파일로 저장\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "with open(\"./data/train_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"./data/val_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 기존처럼 dataset 생성\n",
    "train_dataset = QuestionDataset(\"./data/train_split.json\", tokenizer)\n",
    "val_dataset = QuestionDataset(\"./data/val_split.json\", tokenizer)\n",
    "\n",
    "# 각각의 DataLoader 구성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3UxGu3UysOkK",
   "metadata": {
    "id": "3UxGu3UysOkK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd7efbd1-373e-45f7-842c-feb385b6cbb1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 1] Training: 100%|██████████| 26/26 [00:15<00:00,  1.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 1] Train Loss: 23.6828\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 1] Validation: 100%|██████████| 7/7 [00:01<00:00,  6.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 1] Val Loss: 0.7658, F1 Score: 0.9805\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 2] Training: 100%|██████████| 26/26 [00:14<00:00,  1.74it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 2] Train Loss: 2.5481\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 2] Validation: 100%|██████████| 7/7 [00:01<00:00,  6.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 2] Val Loss: 0.1872, F1 Score: 0.9900\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 3] Training: 100%|██████████| 26/26 [00:15<00:00,  1.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 3] Train Loss: 1.8548\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 3] Validation: 100%|██████████| 7/7 [00:01<00:00,  6.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 3] Val Loss: 0.4538, F1 Score: 0.9798\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 4] Training: 100%|██████████| 26/26 [00:15<00:00,  1.70it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 4] Train Loss: 1.6704\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 4] Validation: 100%|██████████| 7/7 [00:01<00:00,  6.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 4] Val Loss: 1.4463, F1 Score: 0.9311\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 5] Training: 100%|██████████| 26/26 [00:15<00:00,  1.68it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 5] Train Loss: 1.4594\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 5] Validation: 100%|██████████| 7/7 [00:01<00:00,  6.13it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 5] Val Loss: 0.7705, F1 Score: 0.9615\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 6] Training: 100%|██████████| 26/26 [00:15<00:00,  1.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 6] Train Loss: 0.6587\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Epoch 6] Validation: 100%|██████████| 7/7 [00:01<00:00,  5.76it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 6] Val Loss: 0.4727, F1 Score: 0.9902\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for epoch in range(6):\n",
    "    # 학습 단계\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"[Epoch {epoch+1}] Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38fdc812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38fdc812",
    "outputId": "90f0bb18-da42-4c3c-b3b6-22dd9c8caf11"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ cls_output.json 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test_cls.json 예측 후 cls_output.json 저장\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# test 데이터 로드\n",
    "with open(\"./data/test_cls.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_questions = [x[\"question\"] for x in test_data]\n",
    "\n",
    "# 토큰화\n",
    "tokenized = tokenizer(\n",
    "    test_questions,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized)\n",
    "    preds = torch.argmax(outputs, dim=-1).tolist()\n",
    "\n",
    "# 결과 저장\n",
    "cls_output = [{\"question\": q, \"label\": l} for q, l in zip(test_questions, preds)]\n",
    "with open(\"outputs/cls_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cls_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"cls_output.json 저장 완료!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
