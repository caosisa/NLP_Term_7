{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ef49df",
   "metadata": {
    "id": "d7ef49df"
   },
   "source": [
    "# 📘 KoBERT 기반 질문 분류기 (Question Classifier)\n",
    "[구글 코랩 전용]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "!sudo apt update -y\n",
    "!sudo apt install python3.10.12 -y\n",
    "\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10.12 2\n",
    "!sudo update-alternatives --config python3\n",
    "\n",
    "!sudo apt install python3-pip -y\n",
    "\"\"\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djgq86qojaZ4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750421187149,
     "user_tz": -540,
     "elapsed": 194830,
     "user": {
      "displayName": "이종현",
      "userId": "09581388165108865634"
     }
    },
    "outputId": "c0942f84-7f17-4e63-c875-0ab2732ee7c7",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "djgq86qojaZ4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!python --version\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzIe60CmEzD5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750421187263,
     "user_tz": -540,
     "elapsed": 108,
     "user": {
      "displayName": "이종현",
      "userId": "09581388165108865634"
     }
    },
    "outputId": "649bf8a5-7c3b-46f1-a884-d85792e6aa86",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "EzIe60CmEzD5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432db6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1432db6c",
    "outputId": "e80e9f1c-a31e-49d2-d78b-86e4d9bcb900"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: line 1: 2.0: No such file or directory\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m137.6/137.6 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.0/9.0 MB\u001B[0m \u001B[31m91.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m102.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m244.2/244.2 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m91.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m77.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m57.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m7.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy<2.0\n",
    "!pip install -q datasets pandas\n",
    "!pip install -q transformers==4.40.0\n",
    "!pip install -q accelerate==0.21.0\n",
    "!pip install -q torch==2.5.1\n",
    "!pip install -q gluonnlp==0.10.0\n",
    "!pip install -q kobert-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf788f91",
   "metadata": {
    "id": "cf788f91"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278563d",
   "metadata": {
    "id": "f278563d"
   },
   "outputs": [],
   "source": [
    "class QClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(QClassifier, self).__init__()\n",
    "        self.basemodel = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.basemodel.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.basemodel(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "model = QClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecf626",
   "metadata": {
    "id": "88ecf626"
   },
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, json_path, tokenizer, max_len=64):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            item[\"question\"],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
    "            \"token_type_ids\": encoded[\"token_type_ids\"].squeeze(),\n",
    "            \"label\": torch.tensor(item[\"label\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767f048",
   "metadata": {
    "id": "5767f048"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 원본 전체 데이터 로드\n",
    "with open(\"/content/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# 80% train, 20% validation split\n",
    "train_data, val_data = train_test_split(full_data, test_size=0.2, stratify=[x[\"label\"] for x in full_data], random_state=42)\n",
    "\n",
    "# 파일로 저장\n",
    "with open(\"/content/train_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"/content/val_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 기존처럼 dataset 생성\n",
    "train_dataset = QuestionDataset(\"/content/train_split.json\", tokenizer)\n",
    "val_dataset = QuestionDataset(\"/content/val_split.json\", tokenizer)\n",
    "\n",
    "# 각각의 DataLoader 구성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for epoch in range(6):\n",
    "    # 🔹 학습 단계\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # 🔹 검증 단계\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"[Epoch {epoch+1}] Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f}, F1 Score: {f1:.4f}\")"
   ],
   "metadata": {
    "id": "3UxGu3UysOkK"
   },
   "id": "3UxGu3UysOkK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_accuracy(model, tokenizer, test_path, label_key=\"label\", device=\"cuda\"):\n",
    "    # 1) 데이터 로드\n",
    "    try:\n",
    "        with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds, trues = [], []\n",
    "    wrong_samples = []  # 오답 저장\n",
    "\n",
    "    for item in test_data:\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                item[\"question\"],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=64,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "            # KoBERT는 token_type_ids가 꼭 있어야 하므로 0으로 채움\n",
    "            encoded[\"token_type_ids\"] = torch.zeros_like(encoded[\"input_ids\"])\n",
    "\n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            token_type_ids = encoded[\"token_type_ids\"].to(device)\n",
    "            label = item[label_key]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids, attention_mask, token_type_ids)\n",
    "                pred_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            preds.append(pred_label)\n",
    "            trues.append(label)\n",
    "\n",
    "            if pred_label != label:\n",
    "                wrong_samples.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"true\": label,\n",
    "                    \"pred\": pred_label\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 샘플 처리 오류 (무시됨): {e}\")\n",
    "            continue\n",
    "\n",
    "    # 정확도 및 리포트 출력\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(f\"\\n✅ Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"🎯 F1 Score (macro): {f1_score(trues, preds, average='macro'):.4f}\")\n",
    "\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(trues, preds, digits=3))\n",
    "\n",
    "    # 오답 샘플 출력\n",
    "    print(f\"\\n❌ 잘못 분류된 문항 수: {len(wrong_samples)} / {len(test_data)}\")\n",
    "    for sample in wrong_samples[:]:  # 최대 10개만 출력\n",
    "        print(f\"Q: {sample['question']}\\n→ 예측: {sample['pred']} / 실제: {sample['true']}\\n\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "evaluate_accuracy(model, tokenizer, test_path=\"/content/test1.json\")"
   ],
   "metadata": {
    "id": "FJQMZ_HNki2c"
   },
   "id": "FJQMZ_HNki2c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "model.eval()  # 평가 모드로 설정\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"질문을 입력하세요 (종료하려면 'exit' 또는 '종료'): \")\n",
    "\n",
    "    if user_input.strip().lower() in [\"exit\", \"종료\"]:\n",
    "        print(\"테스트를 종료합니다.\")\n",
    "        break\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        user_input,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    token_type_ids = encoded.get(\"token_type_ids\", torch.zeros_like(input_ids)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    label_map = {\n",
    "        0: \"졸업 요건\",\n",
    "        1: \"공지사항\",\n",
    "        2: \"학사 일정\",\n",
    "        3: \"식단 안내\",\n",
    "        4: \"셔틀버스/통학\"\n",
    "    }\n",
    "    print(f\"🔍 예측된 라벨: {predicted_label} ({label_map[predicted_label]})\\n\")"
   ],
   "metadata": {
    "id": "NiN3vgmkXM1N"
   },
   "id": "NiN3vgmkXM1N",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
