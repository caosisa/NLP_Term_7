{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ef49df",
   "metadata": {
    "id": "d7ef49df"
   },
   "source": [
    "# ğŸ“˜ KoBERT ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜ê¸° (Question Classifier)\n",
    "[êµ¬ê¸€ ì½”ë© ì „ìš©]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "!sudo apt update -y\n",
    "!sudo apt install python3.10.12 -y\n",
    "\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10.12 2\n",
    "!sudo update-alternatives --config python3\n",
    "\n",
    "!sudo apt install python3-pip -y\n",
    "\"\"\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djgq86qojaZ4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750421187149,
     "user_tz": -540,
     "elapsed": 194830,
     "user": {
      "displayName": "ì´ì¢…í˜„",
      "userId": "09581388165108865634"
     }
    },
    "outputId": "c0942f84-7f17-4e63-c875-0ab2732ee7c7",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "djgq86qojaZ4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!python --version\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzIe60CmEzD5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750421187263,
     "user_tz": -540,
     "elapsed": 108,
     "user": {
      "displayName": "ì´ì¢…í˜„",
      "userId": "09581388165108865634"
     }
    },
    "outputId": "649bf8a5-7c3b-46f1-a884-d85792e6aa86",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "EzIe60CmEzD5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432db6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1432db6c",
    "outputId": "e80e9f1c-a31e-49d2-d78b-86e4d9bcb900"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: line 1: 2.0: No such file or directory\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m137.6/137.6 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m9.0/9.0 MB\u001B[0m \u001B[31m91.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m102.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m244.2/244.2 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m91.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m77.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m57.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m7.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy<2.0\n",
    "!pip install -q datasets pandas\n",
    "!pip install -q transformers==4.40.0\n",
    "!pip install -q accelerate==0.21.0\n",
    "!pip install -q torch==2.5.1\n",
    "!pip install -q gluonnlp==0.10.0\n",
    "!pip install -q kobert-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf788f91",
   "metadata": {
    "id": "cf788f91"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278563d",
   "metadata": {
    "id": "f278563d"
   },
   "outputs": [],
   "source": [
    "class QClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(QClassifier, self).__init__()\n",
    "        self.basemodel = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.basemodel.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.basemodel(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "model = QClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecf626",
   "metadata": {
    "id": "88ecf626"
   },
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, json_path, tokenizer, max_len=64):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            item[\"question\"],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
    "            \"token_type_ids\": encoded[\"token_type_ids\"].squeeze(),\n",
    "            \"label\": torch.tensor(item[\"label\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767f048",
   "metadata": {
    "id": "5767f048"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ì›ë³¸ ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "with open(\"/content/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# 80% train, 20% validation split\n",
    "train_data, val_data = train_test_split(full_data, test_size=0.2, stratify=[x[\"label\"] for x in full_data], random_state=42)\n",
    "\n",
    "# íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(\"/content/train_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"/content/val_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ê¸°ì¡´ì²˜ëŸ¼ dataset ìƒì„±\n",
    "train_dataset = QuestionDataset(\"/content/train_split.json\", tokenizer)\n",
    "val_dataset = QuestionDataset(\"/content/val_split.json\", tokenizer)\n",
    "\n",
    "# ê°ê°ì˜ DataLoader êµ¬ì„±\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for epoch in range(6):\n",
    "    # ğŸ”¹ í•™ìŠµ ë‹¨ê³„\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # ğŸ”¹ ê²€ì¦ ë‹¨ê³„\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"[Epoch {epoch+1}] Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f}, F1 Score: {f1:.4f}\")"
   ],
   "metadata": {
    "id": "3UxGu3UysOkK"
   },
   "id": "3UxGu3UysOkK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_accuracy(model, tokenizer, test_path, label_key=\"label\", device=\"cuda\"):\n",
    "    # 1) ë°ì´í„° ë¡œë“œ\n",
    "    try:\n",
    "        with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds, trues = [], []\n",
    "    wrong_samples = []  # ì˜¤ë‹µ ì €ì¥\n",
    "\n",
    "    for item in test_data:\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                item[\"question\"],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=64,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "            # KoBERTëŠ” token_type_idsê°€ ê¼­ ìˆì–´ì•¼ í•˜ë¯€ë¡œ 0ìœ¼ë¡œ ì±„ì›€\n",
    "            encoded[\"token_type_ids\"] = torch.zeros_like(encoded[\"input_ids\"])\n",
    "\n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            token_type_ids = encoded[\"token_type_ids\"].to(device)\n",
    "            label = item[label_key]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids, attention_mask, token_type_ids)\n",
    "                pred_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            preds.append(pred_label)\n",
    "            trues.append(label)\n",
    "\n",
    "            if pred_label != label:\n",
    "                wrong_samples.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"true\": label,\n",
    "                    \"pred\": pred_label\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ìƒ˜í”Œ ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}\")\n",
    "            continue\n",
    "\n",
    "    # ì •í™•ë„ ë° ë¦¬í¬íŠ¸ ì¶œë ¥\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(f\"\\nâœ… Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"ğŸ¯ F1 Score (macro): {f1_score(trues, preds, average='macro'):.4f}\")\n",
    "\n",
    "    print(\"\\nğŸ“Š Classification Report:\")\n",
    "    print(classification_report(trues, preds, digits=3))\n",
    "\n",
    "    # ì˜¤ë‹µ ìƒ˜í”Œ ì¶œë ¥\n",
    "    print(f\"\\nâŒ ì˜ëª» ë¶„ë¥˜ëœ ë¬¸í•­ ìˆ˜: {len(wrong_samples)} / {len(test_data)}\")\n",
    "    for sample in wrong_samples[:]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥\n",
    "        print(f\"Q: {sample['question']}\\nâ†’ ì˜ˆì¸¡: {sample['pred']} / ì‹¤ì œ: {sample['true']}\\n\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "evaluate_accuracy(model, tokenizer, test_path=\"/content/test1.json\")"
   ],
   "metadata": {
    "id": "FJQMZ_HNki2c"
   },
   "id": "FJQMZ_HNki2c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ'): \")\n",
    "\n",
    "    if user_input.strip().lower() in [\"exit\", \"ì¢…ë£Œ\"]:\n",
    "        print(\"í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        user_input,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    token_type_ids = encoded.get(\"token_type_ids\", torch.zeros_like(input_ids)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    label_map = {\n",
    "        0: \"ì¡¸ì—… ìš”ê±´\",\n",
    "        1: \"ê³µì§€ì‚¬í•­\",\n",
    "        2: \"í•™ì‚¬ ì¼ì •\",\n",
    "        3: \"ì‹ë‹¨ ì•ˆë‚´\",\n",
    "        4: \"ì…”í‹€ë²„ìŠ¤/í†µí•™\"\n",
    "    }\n",
    "    print(f\"ğŸ” ì˜ˆì¸¡ëœ ë¼ë²¨: {predicted_label} ({label_map[predicted_label]})\\n\")"
   ],
   "metadata": {
    "id": "NiN3vgmkXM1N"
   },
   "id": "NiN3vgmkXM1N",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
